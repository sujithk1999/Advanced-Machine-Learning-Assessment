{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5361184f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5361184f",
    "outputId": "dc1f95bb-a3b5-40eb-98d2-68c92c7a3088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.9.1-cp38-cp38-win_amd64.whl (444.1 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.44.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.19.4)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (20.9)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.20.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (52.0.0.post20210125)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.36.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.6.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (3.3.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-gpu) (2.4.7)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=2a6d9e55505f4b4653730c4a41eaa9a79f101d1f7e69fb21cb8923d125952c89\n",
      "  Stored in directory: c:\\users\\computing\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow-gpu\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.8.0\n",
      "    Uninstalling tensorboard-2.8.0:\n",
      "      Successfully uninstalled tensorboard-2.8.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-pasta-0.2.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.6 opt-einsum-3.3.0 tensorboard-2.9.1 tensorflow-estimator-2.9.0 tensorflow-gpu-2.9.1 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27dbc7",
   "metadata": {
    "id": "4d27dbc7"
   },
   "source": [
    "**Installing the game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73aa6aba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73aa6aba",
    "outputId": "be2fbacc-3a20-464e-8485-118ab0c2f1ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym_super_mario_bros==7.3.0\n",
      "  Downloading gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198 kB)\n",
      "Collecting nes_py\n",
      "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
      "Collecting gym>=0.17.2\n",
      "  Downloading gym-0.25.1.tar.gz (732 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes_py) (1.20.1)\n",
      "Collecting pyglet<=1.5.21,>=1.4.0\n",
      "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes_py) (4.59.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (4.11.3)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes_py) (3.4.1)\n",
      "Building wheels for collected packages: nes-py, gym\n",
      "  Building wheel for nes-py (setup.py): started\n",
      "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
      "  Created wheel for nes-py: filename=nes_py-8.2.1-cp38-cp38-win_amd64.whl size=48249 sha256=35034dc1ec210bcfc5da8d49a0d9570de20c7d5a47026c2b658c25e5a810edd1\n",
      "  Stored in directory: c:\\users\\computing\\appdata\\local\\pip\\cache\\wheels\\17\\e5\\5c\\8dfae61b44dbf56c458483aa09accef55a650e0527f6cbd872\n",
      "  Building wheel for gym (PEP 517): started\n",
      "  Building wheel for gym (PEP 517): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.25.1-py3-none-any.whl size=849027 sha256=1f9878a82d33a14db6ac28ab77a509c92dc87f757245a2c1bdbb935cdfde51cc\n",
      "  Stored in directory: c:\\users\\computing\\appdata\\local\\pip\\cache\\wheels\\cc\\73\\13\\48bbc89268e1515e9e26f60c50ee870632c700c75cf7a53fbe\n",
      "Successfully built nes-py gym\n",
      "Installing collected packages: gym-notices, pyglet, gym, nes-py, gym-super-mario-bros\n",
      "Successfully installed gym-0.25.1 gym-notices-0.0.7 gym-super-mario-bros-7.3.0 nes-py-8.2.1 pyglet-1.5.21\n"
     ]
    }
   ],
   "source": [
    "!pip install gym_super_mario_bros==7.3.0 nes_py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c23fdf",
   "metadata": {
    "id": "44c23fdf"
   },
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbe586e",
   "metadata": {
    "id": "1bbe586e"
   },
   "outputs": [],
   "source": [
    "# Importing the game\n",
    "import gym_super_mario_bros\n",
    "# Importing the Joypad wrapper\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Importing the SIMPLIFIED controls\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78a3fed",
   "metadata": {
    "id": "f78a3fed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import gym_super_mario_bros\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c352577",
   "metadata": {
    "id": "8c352577"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408beea3",
   "metadata": {
    "id": "408beea3"
   },
   "source": [
    "**Grayscaling for observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14813e10",
   "metadata": {
    "id": "14813e10"
   },
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cff57b",
   "metadata": {
    "id": "98cff57b"
   },
   "source": [
    "**Resizing the observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcfff7a",
   "metadata": {
    "id": "abcfff7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros2-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)\n",
    "\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros2-v0')\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "env.seed(64)\n",
    "env.action_space.seed(64)\n",
    "torch.manual_seed(64)\n",
    "torch.random.manual_seed(64)\n",
    "np.random.seed(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0882fbb6",
   "metadata": {
    "id": "0882fbb6"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return Categorical(logits=self.actor(obs)), self.critic(obs).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df6079f",
   "metadata": {
    "id": "8df6079f"
   },
   "outputs": [],
   "source": [
    "class PPOSolver:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.gamma = 0.95\n",
    "        self.lamda = 0.95\n",
    "        self.worker_steps = 4096\n",
    "        self.n_mini_batch = 4\n",
    "        self.epochs = 30\n",
    "        self.save_directory = r\"C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\"\n",
    "        self.batch_size = self.worker_steps\n",
    "        self.mini_batch_size = self.batch_size // self.n_mini_batch\n",
    "        self.obs = env.reset().__array__()\n",
    "        self.policy = Model().to(device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': 0.00025},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': 0.001}\n",
    "        ], eps=1e-4)\n",
    "        self.policy_old = Model().to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.all_episode_rewards = []\n",
    "        self.all_mean_rewards = []\n",
    "        self.episode = 0\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(self.episode))\n",
    "        torch.save(self.policy_old.state_dict(), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        self.policy.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        self.policy_old.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        print('Resuming training from checkpoint \\'{}\\'.'.format(filename))\n",
    "\n",
    "    def sample(self):\n",
    "        rewards = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        actions = np.zeros(self.worker_steps, dtype=np.int32)\n",
    "        done = np.zeros(self.worker_steps, dtype=bool)\n",
    "        obs = np.zeros((self.worker_steps, 4, 84, 84), dtype=np.float32)\n",
    "        log_pis = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        values = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        for t in range(self.worker_steps):\n",
    "            with torch.no_grad():\n",
    "                obs[t] = self.obs\n",
    "                pi, v = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "                values[t] = v.cpu().numpy()\n",
    "                a = pi.sample()\n",
    "                actions[t] = a.cpu().numpy()\n",
    "                log_pis[t] = pi.log_prob(a).cpu().numpy()\n",
    "            self.obs, rewards[t], done[t], _ = env.step(actions[t])\n",
    "            self.obs = self.obs.__array__()\n",
    "            env.render()\n",
    "            self.rewards.append(rewards[t])\n",
    "            if done[t]:\n",
    "                self.episode += 1\n",
    "                self.all_episode_rewards.append(np.sum(self.rewards))\n",
    "                self.rewards = []\n",
    "                env.reset()\n",
    "                if self.episode % 10 == 0:\n",
    "                    print('Episode: {}, average reward: {}'.format(self.episode, np.mean(self.all_episode_rewards[-10:])))\n",
    "                    self.all_mean_rewards.append(np.mean(self.all_episode_rewards[-10:]))\n",
    "                    plt.plot(self.all_mean_rewards)\n",
    "                    plt.savefig(\"{}/mean_reward_{}.png\".format(self.save_directory, self.episode))\n",
    "                    plt.clf()\n",
    "                    self.save_checkpoint()\n",
    "        returns, advantages = self.calculate_advantages(done, rewards, values)\n",
    "        return {\n",
    "            'obs': torch.tensor(obs.reshape(obs.shape[0], *obs.shape[1:]), dtype=torch.float32, device=device),\n",
    "            'actions': torch.tensor(actions, device=device),\n",
    "            'values': torch.tensor(values, device=device),\n",
    "            'log_pis': torch.tensor(log_pis, device=device),\n",
    "            'advantages': torch.tensor(advantages, device=device, dtype=torch.float32),\n",
    "            'returns': torch.tensor(returns, device=device, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "    def calculate_advantages(self, done, rewards, values):\n",
    "        _, last_value = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "        last_value = last_value.cpu().data.numpy()\n",
    "        values = np.append(values, last_value)\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            mask = 1.0 - done[i]\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] * mask - values[i]\n",
    "            gae = delta + self.gamma * self.lamda * mask * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "        adv = np.array(returns) - values[:-1]\n",
    "        return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "\n",
    "    def train(self, samples, clip_range):\n",
    "        indexes = torch.randperm(self.batch_size)\n",
    "        for start in range(0, self.batch_size, self.mini_batch_size):\n",
    "            end = start + self.mini_batch_size\n",
    "            mini_batch_indexes = indexes[start: end]\n",
    "            mini_batch = {}\n",
    "            for k, v in samples.items():\n",
    "                mini_batch[k] = v[mini_batch_indexes]\n",
    "            for _ in range(self.epochs):\n",
    "                loss = self.calculate_loss(clip_range=clip_range, samples=mini_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "    def calculate_loss(self, samples, clip_range):\n",
    "        sampled_returns = samples['returns']\n",
    "        sampled_advantages = samples['advantages']\n",
    "        pi, value = self.policy(samples['obs'])\n",
    "        ratio = torch.exp(pi.log_prob(samples['actions']) - samples['log_pis'])\n",
    "        clipped_ratio = ratio.clamp(min=1.0 - clip_range, max=1.0 + clip_range)\n",
    "        policy_reward = torch.min(ratio * sampled_advantages, clipped_ratio * sampled_advantages)\n",
    "        entropy_bonus = pi.entropy()\n",
    "        vf_loss = self.mse_loss(value, sampled_returns)\n",
    "        loss = -policy_reward + 0.5 * vf_loss - 0.01 * entropy_bonus\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f133e",
   "metadata": {
    "id": "989f133e"
   },
   "source": [
    "**Using PPO agent as solver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05317a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "c05317a6",
    "outputId": "4b7e9723-697b-4c3b-f086-fa3d6f0d0c4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, average reward: 1527.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_10.pth'\n",
      "Episode: 20, average reward: 1653.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_20.pth'\n",
      "Episode: 30, average reward: 2126.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_30.pth'\n",
      "Episode: 40, average reward: 1877.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_40.pth'\n",
      "Episode: 50, average reward: 1777.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_50.pth'\n",
      "Episode: 60, average reward: 1754.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_60.pth'\n",
      "Episode: 70, average reward: 1849.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_70.pth'\n",
      "Episode: 80, average reward: 1983.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_80.pth'\n",
      "Episode: 90, average reward: 1885.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_90.pth'\n",
      "Episode: 100, average reward: 2232.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_100.pth'\n",
      "Episode: 110, average reward: 1997.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_110.pth'\n",
      "Episode: 120, average reward: 2371.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_120.pth'\n",
      "Episode: 130, average reward: 2181.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_130.pth'\n",
      "Episode: 140, average reward: 2315.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_140.pth'\n",
      "Episode: 150, average reward: 2315.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_150.pth'\n",
      "Episode: 160, average reward: 2176.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_160.pth'\n",
      "Episode: 170, average reward: 2249.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_170.pth'\n",
      "Episode: 180, average reward: 2369.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_180.pth'\n",
      "Episode: 190, average reward: 2381.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_190.pth'\n",
      "Episode: 200, average reward: 2689.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_200.pth'\n",
      "Episode: 210, average reward: 2562.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_210.pth'\n",
      "Episode: 220, average reward: 2529.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_220.pth'\n",
      "Episode: 230, average reward: 2532.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_230.pth'\n",
      "Episode: 240, average reward: 2599.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_240.pth'\n",
      "Episode: 250, average reward: 2490.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_250.pth'\n",
      "Episode: 260, average reward: 2412.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_260.pth'\n",
      "Episode: 270, average reward: 2570.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_270.pth'\n",
      "Episode: 280, average reward: 2717.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_280.pth'\n",
      "Episode: 290, average reward: 2612.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_290.pth'\n",
      "Episode: 300, average reward: 2662.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_300.pth'\n",
      "Episode: 310, average reward: 2761.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_310.pth'\n",
      "Episode: 320, average reward: 2697.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_320.pth'\n",
      "Episode: 330, average reward: 2915.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_330.pth'\n",
      "Episode: 340, average reward: 2810.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_340.pth'\n",
      "Episode: 350, average reward: 3047.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_350.pth'\n",
      "Episode: 360, average reward: 2756.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_360.pth'\n",
      "Episode: 370, average reward: 2768.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_370.pth'\n",
      "Episode: 380, average reward: 2591.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_380.pth'\n",
      "Episode: 390, average reward: 2704.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_390.pth'\n",
      "Episode: 400, average reward: 2705.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_400.pth'\n",
      "Episode: 410, average reward: 2380.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_410.pth'\n",
      "Episode: 420, average reward: 2884.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_420.pth'\n",
      "Episode: 430, average reward: 2563.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_430.pth'\n",
      "Episode: 440, average reward: 2795.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_440.pth'\n",
      "Episode: 450, average reward: 2544.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_450.pth'\n",
      "Episode: 460, average reward: 2521.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_460.pth'\n",
      "Episode: 470, average reward: 2631.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_470.pth'\n",
      "Episode: 480, average reward: 2778.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_480.pth'\n",
      "Episode: 490, average reward: 2704.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_490.pth'\n",
      "Episode: 500, average reward: 2730.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_500.pth'\n",
      "Episode: 510, average reward: 2553.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_510.pth'\n",
      "Episode: 520, average reward: 2385.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_520.pth'\n",
      "Episode: 530, average reward: 2490.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_530.pth'\n",
      "Episode: 540, average reward: 2896.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_540.pth'\n",
      "Episode: 550, average reward: 2843.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_550.pth'\n",
      "Episode: 560, average reward: 3014.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_560.pth'\n",
      "Episode: 570, average reward: 2730.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_570.pth'\n",
      "Episode: 580, average reward: 2792.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_580.pth'\n",
      "Episode: 590, average reward: 2608.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_590.pth'\n",
      "Episode: 600, average reward: 2623.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_600.pth'\n",
      "Episode: 610, average reward: 2684.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_610.pth'\n",
      "Episode: 620, average reward: 3033.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_620.pth'\n",
      "Episode: 630, average reward: 3276.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_630.pth'\n",
      "Episode: 640, average reward: 3393.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_640.pth'\n",
      "Episode: 650, average reward: 3186.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_650.pth'\n",
      "Episode: 660, average reward: 3031.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_660.pth'\n",
      "Episode: 670, average reward: 2613.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_670.pth'\n",
      "Episode: 680, average reward: 2762.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_680.pth'\n",
      "Episode: 690, average reward: 2779.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_690.pth'\n",
      "Episode: 700, average reward: 3257.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_700.pth'\n",
      "Episode: 710, average reward: 3021.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_710.pth'\n",
      "Episode: 720, average reward: 2565.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_720.pth'\n",
      "Episode: 730, average reward: 2722.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_730.pth'\n",
      "Episode: 740, average reward: 3046.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_740.pth'\n",
      "Episode: 750, average reward: 3093.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_750.pth'\n",
      "Episode: 760, average reward: 3017.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_760.pth'\n",
      "Episode: 770, average reward: 2958.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_770.pth'\n",
      "Episode: 780, average reward: 3106.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_780.pth'\n",
      "Episode: 790, average reward: 3100.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_790.pth'\n",
      "Episode: 800, average reward: 2745.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_800.pth'\n",
      "Episode: 810, average reward: 2873.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_810.pth'\n",
      "Episode: 820, average reward: 2844.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_820.pth'\n",
      "Episode: 830, average reward: 3173.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_830.pth'\n",
      "Episode: 840, average reward: 3198.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_840.pth'\n",
      "Episode: 850, average reward: 3072.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_850.pth'\n",
      "Episode: 860, average reward: 2582.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_860.pth'\n",
      "Episode: 870, average reward: 2833.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_870.pth'\n",
      "Episode: 880, average reward: 3158.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_880.pth'\n",
      "Episode: 890, average reward: 3172.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_890.pth'\n",
      "Episode: 900, average reward: 3310.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_900.pth'\n",
      "Episode: 910, average reward: 3311.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_910.pth'\n",
      "Episode: 920, average reward: 3247.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_920.pth'\n",
      "Episode: 930, average reward: 3291.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_930.pth'\n",
      "Episode: 940, average reward: 3005.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_940.pth'\n",
      "Episode: 950, average reward: 3262.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_950.pth'\n",
      "Episode: 960, average reward: 3381.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_960.pth'\n",
      "Episode: 970, average reward: 3448.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_970.pth'\n",
      "Episode: 980, average reward: 3133.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_980.pth'\n",
      "Episode: 990, average reward: 3267.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_990.pth'\n",
      "Episode: 1000, average reward: 3203.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO SEE=64 WITHOUTBASELINE\\checkpoint_1000.pth'\n"
     ]
    }
   ],
   "source": [
    "solver = PPOSolver()\n",
    "while True:\n",
    "    solver.train(solver.sample(), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9751983",
   "metadata": {
    "id": "e9751983"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4597b3",
   "metadata": {
    "id": "ba4597b3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9dc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PPO agent seed 42 DRL agent withoutbaseline.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
